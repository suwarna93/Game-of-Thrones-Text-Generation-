{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport re\nfrom nltk.corpus import stopwords\nimport string\n\nimport keras\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\n\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:16:15.049627Z","iopub.execute_input":"2022-05-08T09:16:15.050431Z","iopub.status.idle":"2022-05-08T09:16:20.648211Z","shell.execute_reply.started":"2022-05-08T09:16:15.050389Z","shell.execute_reply":"2022-05-08T09:16:20.647376Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def load_data(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\ndata = load_data(\"../input/game-of-thrones-text-generation/got1.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:16:20.650193Z","iopub.execute_input":"2022-05-08T09:16:20.650464Z","iopub.status.idle":"2022-05-08T09:16:20.691466Z","shell.execute_reply.started":"2022-05-08T09:16:20.650436Z","shell.execute_reply":"2022-05-08T09:16:20.690749Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data[:1000]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:16:20.693774Z","iopub.execute_input":"2022-05-08T09:16:20.694042Z","iopub.status.idle":"2022-05-08T09:16:20.703640Z","shell.execute_reply.started":"2022-05-08T09:16:20.694006Z","shell.execute_reply":"2022-05-08T09:16:20.702265Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# data cleaning process\nimport re                                # Regular expressions to use sub function for replacing the useless text from the data\n\ndef clean_text(text):\n    text = re.sub(r',', '', text)\n    text = re.sub(r'\\'', '',  text)\n    text = re.sub(r'\\\"', '', text)\n    text = re.sub(r'\\(', '', text)\n    text = re.sub(r'\\)', '', text)\n    text = re.sub(r'\\n', '', text)\n    text = re.sub(r'“', '', text)\n    text = re.sub(r'”', '', text)\n    text = re.sub(r'’', '', text)\n    text = re.sub(r'\\.', '', text)\n    text = re.sub(r';', '', text)\n    text = re.sub(r':', '', text)\n    text = re.sub(r'\\-', '', text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:16:20.706125Z","iopub.execute_input":"2022-05-08T09:16:20.706433Z","iopub.status.idle":"2022-05-08T09:16:20.716531Z","shell.execute_reply.started":"2022-05-08T09:16:20.706392Z","shell.execute_reply":"2022-05-08T09:16:20.715863Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# cleaning the data\nlower_data = data.lower()           # Converting the string to lower case to get uniformity\n\nsplit_data = lower_data.splitlines()      # Splitting the data to get every line seperately but this will give the list of uncleaned data\n\nsplit_data[:50] ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:16:20.717947Z","iopub.execute_input":"2022-05-08T09:16:20.718762Z","iopub.status.idle":"2022-05-08T09:16:20.735249Z","shell.execute_reply.started":"2022-05-08T09:16:20.718687Z","shell.execute_reply":"2022-05-08T09:16:20.734439Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"final = ''                          # initiating a argument with blank string to hold the values of final cleaned data\n\nfor line in split_data:\n    line = clean_text(line)\n    final += '\\n' + line\n\nprint(final)[:10]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:16:20.736555Z","iopub.execute_input":"2022-05-08T09:16:20.737392Z","iopub.status.idle":"2022-05-08T09:16:21.353179Z","shell.execute_reply.started":"2022-05-08T09:16:20.737364Z","shell.execute_reply":"2022-05-08T09:16:21.350993Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"final_data = final.split('\\n')       # splitting again to get list of cleaned and splitted data ready to be processed\nfinal_data[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:17:19.449666Z","iopub.execute_input":"2022-05-08T09:17:19.449991Z","iopub.status.idle":"2022-05-08T09:17:19.459837Z","shell.execute_reply.started":"2022-05-08T09:17:19.449958Z","shell.execute_reply":"2022-05-08T09:17:19.459136Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:17:23.350553Z","iopub.execute_input":"2022-05-08T09:17:23.351361Z","iopub.status.idle":"2022-05-08T09:17:23.357287Z","shell.execute_reply.started":"2022-05-08T09:17:23.351319Z","shell.execute_reply":"2022-05-08T09:17:23.356547Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Instantiating the Tokenizer\nmax_vocab = 100\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(final_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:17:47.968326Z","iopub.execute_input":"2022-05-08T09:17:47.968629Z","iopub.status.idle":"2022-05-08T09:17:48.332392Z","shell.execute_reply.started":"2022-05-08T09:17:47.968596Z","shell.execute_reply":"2022-05-08T09:17:48.331628Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Getting the total number of words of the data.\nword2idx = tokenizer.word_index\nprint(len(word2idx))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:17:50.574404Z","iopub.execute_input":"2022-05-08T09:17:50.574810Z","iopub.status.idle":"2022-05-08T09:17:50.581324Z","shell.execute_reply.started":"2022-05-08T09:17:50.574722Z","shell.execute_reply":"2022-05-08T09:17:50.580544Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nvocab_size = len(word2idx) + 1        # Adding 1 to the vocab_size because the index starts from 1 not 0. This will make it uniform when using it further\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:18:17.847664Z","iopub.execute_input":"2022-05-08T09:18:17.848515Z","iopub.status.idle":"2022-05-08T09:18:17.854558Z","shell.execute_reply.started":"2022-05-08T09:18:17.848466Z","shell.execute_reply":"2022-05-08T09:18:17.853690Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***# ## Creating n-gram sequences from the sentences\n# # # # # # We will turn the sentences to sequences line by line and create n_gram sequences*******","metadata":{}},{"cell_type":"code","source":"input_seq = []\n\nfor line in final_data:\n  token_list = tokenizer.texts_to_sequences([line])[0]\n  for i in range(1, len(token_list)):\n    n_gram_seq = token_list[:i+1]\n    input_seq.append(n_gram_seq)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:22:39.478385Z","iopub.execute_input":"2022-05-08T09:22:39.478772Z","iopub.status.idle":"2022-05-08T09:22:40.061993Z","shell.execute_reply.started":"2022-05-08T09:22:39.478708Z","shell.execute_reply":"2022-05-08T09:22:40.061123Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Getting the maximum length of sequence for padding purpose\nmax_seq_length = max(len(x) for x in input_seq)\nprint(max_seq_length)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:23:35.712110Z","iopub.execute_input":"2022-05-08T09:23:35.712945Z","iopub.status.idle":"2022-05-08T09:23:35.729330Z","shell.execute_reply.started":"2022-05-08T09:23:35.712904Z","shell.execute_reply":"2022-05-08T09:23:35.728572Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Padding the sequences and converting them to array\ninput_seq = np.array(pad_sequences(input_seq, maxlen=max_seq_length, padding='pre'))\nprint(input_seq)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:23:38.018877Z","iopub.execute_input":"2022-05-08T09:23:38.019475Z","iopub.status.idle":"2022-05-08T09:23:38.724655Z","shell.execute_reply.started":"2022-05-08T09:23:38.019434Z","shell.execute_reply":"2022-05-08T09:23:38.723876Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Taking xs and labels to train the model.\n\nxs = input_seq[:, :-1]        # xs contains every word in sentence except the last one because we are using this value to predict the y value\nlabels = input_seq[:, -1]     # labels contains only the last word of the sentence which will help in hot encoding the y value in next step\nprint(\"xs: \",xs)\nprint(\"labels:\",labels)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:25:19.783026Z","iopub.execute_input":"2022-05-08T09:25:19.783675Z","iopub.status.idle":"2022-05-08T09:25:19.792969Z","shell.execute_reply.started":"2022-05-08T09:25:19.783636Z","shell.execute_reply":"2022-05-08T09:25:19.792208Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:25:22.253644Z","iopub.execute_input":"2022-05-08T09:25:22.254182Z","iopub.status.idle":"2022-05-08T09:25:22.257909Z","shell.execute_reply.started":"2022-05-08T09:25:22.254143Z","shell.execute_reply":"2022-05-08T09:25:22.257201Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"ys = to_categorical(labels, num_classes=vocab_size)\nprint(ys)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:25:23.997796Z","iopub.execute_input":"2022-05-08T09:25:23.998577Z","iopub.status.idle":"2022-05-08T09:25:24.295958Z","shell.execute_reply.started":"2022-05-08T09:25:23.998525Z","shell.execute_reply":"2022-05-08T09:25:24.295017Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:25:27.464246Z","iopub.execute_input":"2022-05-08T09:25:27.464877Z","iopub.status.idle":"2022-05-08T09:25:27.471284Z","shell.execute_reply.started":"2022-05-08T09:25:27.464835Z","shell.execute_reply":"2022-05-08T09:25:27.470478Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# using the pipeline method of sequential to define a model\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 124, input_length=max_seq_length-1))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(520, return_sequences=True))\nmodel.add(Bidirectional(LSTM(340, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:25:54.797247Z","iopub.execute_input":"2022-05-08T09:25:54.797521Z","iopub.status.idle":"2022-05-08T09:25:55.590524Z","shell.execute_reply.started":"2022-05-08T09:25:54.797491Z","shell.execute_reply":"2022-05-08T09:25:55.589782Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001),\n              loss = 'categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:26:01.283338Z","iopub.execute_input":"2022-05-08T09:26:01.283624Z","iopub.status.idle":"2022-05-08T09:26:01.294326Z","shell.execute_reply.started":"2022-05-08T09:26:01.283592Z","shell.execute_reply":"2022-05-08T09:26:01.293542Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model.summary()    ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:26:21.666097Z","iopub.execute_input":"2022-05-08T09:26:21.666981Z","iopub.status.idle":"2022-05-08T09:26:21.677224Z","shell.execute_reply.started":"2022-05-08T09:26:21.666941Z","shell.execute_reply":"2022-05-08T09:26:21.676243Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"r = model.fit(xs,ys,epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:26:28.974367Z","iopub.execute_input":"2022-05-08T09:26:28.974653Z","iopub.status.idle":"2022-05-08T09:40:07.248634Z","shell.execute_reply.started":"2022-05-08T09:26:28.974622Z","shell.execute_reply":"2022-05-08T09:40:07.247492Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model on accuracy\nimport matplotlib.pyplot as plt\nplt.plot(r.history['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:40:29.503234Z","iopub.execute_input":"2022-05-08T09:40:29.503560Z","iopub.status.idle":"2022-05-08T09:40:29.711097Z","shell.execute_reply.started":"2022-05-08T09:40:29.503524Z","shell.execute_reply":"2022-05-08T09:40:29.710393Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Defining a function to take input of seed text from user and no. of words to be predicted\n\ndef predict_words(seed, no_words):\n  for i in range(no_words):\n    token_list = tokenizer.texts_to_sequences([seed])[0]\n    token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n    predicted = np.argmax(model.predict(token_list), axis=1)\n    \n    new_word = ''\n\n    for word, index in tokenizer.word_index.items():\n      if predicted == index:\n        new_word = word\n      break\n    seed += \" \" + new_word\n    print(seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:44:21.524437Z","iopub.execute_input":"2022-05-08T09:44:21.524819Z","iopub.status.idle":"2022-05-08T09:44:21.533387Z","shell.execute_reply.started":"2022-05-08T09:44:21.524775Z","shell.execute_reply":"2022-05-08T09:44:21.532537Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# predicting or generating the poem with the seed text\n\nseed_text = 'i am feeling good today'\nnext_words = 20\n\npredict_words(seed_text, next_words)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:09.822472Z","iopub.execute_input":"2022-05-08T09:47:09.822925Z","iopub.status.idle":"2022-05-08T09:47:10.676109Z","shell.execute_reply.started":"2022-05-08T09:47:09.822885Z","shell.execute_reply":"2022-05-08T09:47:10.675226Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# saving the model\n\nmodel.save('text_generator.h5') # Will create a HDF5 file of the model","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:53:17.723189Z","iopub.execute_input":"2022-05-08T09:53:17.723486Z","iopub.status.idle":"2022-05-08T09:53:18.161451Z","shell.execute_reply.started":"2022-05-08T09:53:17.723453Z","shell.execute_reply":"2022-05-08T09:53:18.160669Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}