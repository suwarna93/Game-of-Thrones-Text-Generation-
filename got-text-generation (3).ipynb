{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install keras","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:11.391374Z","iopub.execute_input":"2022-05-20T09:35:11.392444Z","iopub.status.idle":"2022-05-20T09:35:22.211018Z","shell.execute_reply.started":"2022-05-20T09:35:11.392397Z","shell.execute_reply":"2022-05-20T09:35:22.210094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:22.21356Z","iopub.execute_input":"2022-05-20T09:35:22.213869Z","iopub.status.idle":"2022-05-20T09:35:22.2207Z","shell.execute_reply.started":"2022-05-20T09:35:22.213829Z","shell.execute_reply":"2022-05-20T09:35:22.219879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:22.222269Z","iopub.execute_input":"2022-05-20T09:35:22.22256Z","iopub.status.idle":"2022-05-20T09:35:22.235732Z","shell.execute_reply.started":"2022-05-20T09:35:22.222521Z","shell.execute_reply":"2022-05-20T09:35:22.235037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set seeds for reproducability\n\nfrom numpy.random import seed\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:22.239336Z","iopub.execute_input":"2022-05-20T09:35:22.239944Z","iopub.status.idle":"2022-05-20T09:35:22.246827Z","shell.execute_reply.started":"2022-05-20T09:35:22.239912Z","shell.execute_reply":"2022-05-20T09:35:22.246025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport re\nfrom nltk.corpus import stopwords\nimport string\n\nimport keras\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\n\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:22.248369Z","iopub.execute_input":"2022-05-20T09:35:22.248961Z","iopub.status.idle":"2022-05-20T09:35:23.476056Z","shell.execute_reply.started":"2022-05-20T09:35:22.248921Z","shell.execute_reply":"2022-05-20T09:35:23.47494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Let’s demonstrate this with a small pipeline of text preparation including:\n\n### 1)Load the raw text.\n#### 2)Split into tokens.\n#### 3)Convert to lowercase.\n#### 4)Remove punctuation from each token.\n#### 5)Filter out remaining tokens that are not alphabetic.\n#### 6)Filter out tokens that are stop words.\n#### 7)Stem words","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.48046Z","iopub.execute_input":"2022-05-20T09:35:23.480924Z","iopub.status.idle":"2022-05-20T09:35:23.488786Z","shell.execute_reply.started":"2022-05-20T09:35:23.480882Z","shell.execute_reply":"2022-05-20T09:35:23.488068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\ndata = load_data(\"../input/game-of-thrones-text-generation/got1.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.492894Z","iopub.execute_input":"2022-05-20T09:35:23.495255Z","iopub.status.idle":"2022-05-20T09:35:23.511747Z","shell.execute_reply.started":"2022-05-20T09:35:23.495177Z","shell.execute_reply":"2022-05-20T09:35:23.511117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data[:1000])","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.515537Z","iopub.execute_input":"2022-05-20T09:35:23.517488Z","iopub.status.idle":"2022-05-20T09:35:23.526164Z","shell.execute_reply.started":"2022-05-20T09:35:23.517416Z","shell.execute_reply":"2022-05-20T09:35:23.525318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Data Cleaning\n#### Tokenization and Cleaning with NLTK","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.530508Z","iopub.execute_input":"2022-05-20T09:35:23.530914Z","iopub.status.idle":"2022-05-20T09:35:23.537093Z","shell.execute_reply.started":"2022-05-20T09:35:23.530877Z","shell.execute_reply":"2022-05-20T09:35:23.536193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    sample = text\n    sample = re.sub('[%s]' % re.escape(string.punctuation), '', sample)\n    sample = [word for word in sample.split() if word.isalpha()]\n    sample = [word.lower() for word in sample]\n    sample = \" \".join(sample)\n    \n    return sample","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.543831Z","iopub.execute_input":"2022-05-20T09:35:23.54495Z","iopub.status.idle":"2022-05-20T09:35:23.553979Z","shell.execute_reply.started":"2022-05-20T09:35:23.544907Z","shell.execute_reply":"2022-05-20T09:35:23.553307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_data = clean_text(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.558152Z","iopub.execute_input":"2022-05-20T09:35:23.55843Z","iopub.status.idle":"2022-05-20T09:35:23.802597Z","shell.execute_reply.started":"2022-05-20T09:35:23.558396Z","shell.execute_reply":"2022-05-20T09:35:23.801841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_data[:500]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.804Z","iopub.execute_input":"2022-05-20T09:35:23.804537Z","iopub.status.idle":"2022-05-20T09:35:23.811638Z","shell.execute_reply.started":"2022-05-20T09:35:23.804484Z","shell.execute_reply":"2022-05-20T09:35:23.810788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into sentences\nfrom nltk import sent_tokenize\nsentences = sent_tokenize(cleaned_data)\nsentences[:50]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:23.813272Z","iopub.execute_input":"2022-05-20T09:35:23.813783Z","iopub.status.idle":"2022-05-20T09:35:24.114421Z","shell.execute_reply.started":"2022-05-20T09:35:23.813747Z","shell.execute_reply":"2022-05-20T09:35:24.113757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into words\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(cleaned_data)\ntokens[:100]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:24.115657Z","iopub.execute_input":"2022-05-20T09:35:24.118142Z","iopub.status.idle":"2022-05-20T09:35:26.062708Z","shell.execute_reply.started":"2022-05-20T09:35:24.118099Z","shell.execute_reply":"2022-05-20T09:35:26.061931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into words\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(cleaned_data)\n# remove all tokens that are not alphabetic\nwords = [word for word in tokens if word.isalpha()]\nwords[:100]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:26.064149Z","iopub.execute_input":"2022-05-20T09:35:26.064388Z","iopub.status.idle":"2022-05-20T09:35:28.023588Z","shell.execute_reply.started":"2022-05-20T09:35:26.064357Z","shell.execute_reply":"2022-05-20T09:35:28.02286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Filter out Stop Words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:28.024829Z","iopub.execute_input":"2022-05-20T09:35:28.025345Z","iopub.status.idle":"2022-05-20T09:35:28.035054Z","shell.execute_reply.started":"2022-05-20T09:35:28.025303Z","shell.execute_reply":"2022-05-20T09:35:28.034088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stemming of words\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\nstemmed = [porter.stem(word) for word in tokens]\nprint(stemmed[:100])","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:28.036647Z","iopub.execute_input":"2022-05-20T09:35:28.036947Z","iopub.status.idle":"2022-05-20T09:35:34.826376Z","shell.execute_reply.started":"2022-05-20T09:35:28.036883Z","shell.execute_reply":"2022-05-20T09:35:34.825211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_doc = []\nseq_len = 50\nl = seq_len + 1\ntokens = [w for w in cleaned_data.split()]\n\nfor i in range(l, len(tokens)):\n    \n    seq = tokens[i-l:i]\n\n    line = ' '.join(seq)\n    sequences_doc.append(line)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:34.830645Z","iopub.execute_input":"2022-05-20T09:35:34.832691Z","iopub.status.idle":"2022-05-20T09:35:35.495224Z","shell.execute_reply.started":"2022-05-20T09:35:34.832649Z","shell.execute_reply":"2022-05-20T09:35:35.494394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_doc[:100]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:35.496589Z","iopub.execute_input":"2022-05-20T09:35:35.496849Z","iopub.status.idle":"2022-05-20T09:35:35.507788Z","shell.execute_reply.started":"2022-05-20T09:35:35.496812Z","shell.execute_reply":"2022-05-20T09:35:35.506883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sequences_doc)\nsequences = tokenizer.texts_to_sequences(sequences_doc)\n\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:35.509341Z","iopub.execute_input":"2022-05-20T09:35:35.509624Z","iopub.status.idle":"2022-05-20T09:35:57.755007Z","shell.execute_reply.started":"2022-05-20T09:35:35.509587Z","shell.execute_reply":"2022-05-20T09:35:57.754201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = np.array(sequences)\nX, y = sequences[:,:-1], sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)\n\nseq_length = X.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:35:57.756321Z","iopub.execute_input":"2022-05-20T09:35:57.756591Z","iopub.status.idle":"2022-05-20T09:36:00.278344Z","shell.execute_reply.started":"2022-05-20T09:35:57.756556Z","shell.execute_reply":"2022-05-20T09:36:00.277422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_model(vocab_size, seq_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n    model.add(LSTM(20, return_sequences=True))\n    model.add(LSTM(20))\n    model.add(Dense(vocab_size, activation='relu'))\n    model.add(Dense(vocab_size, activation='softmax'))\n    \n    # compile network\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:36:00.279546Z","iopub.execute_input":"2022-05-20T09:36:00.279806Z","iopub.status.idle":"2022-05-20T09:36:00.28698Z","shell.execute_reply.started":"2022-05-20T09:36:00.27977Z","shell.execute_reply":"2022-05-20T09:36:00.286137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = define_model(vocab_size, seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:36:00.288468Z","iopub.execute_input":"2022-05-20T09:36:00.289038Z","iopub.status.idle":"2022-05-20T09:36:00.714652Z","shell.execute_reply.started":"2022-05-20T09:36:00.288936Z","shell.execute_reply":"2022-05-20T09:36:00.713908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X, y, batch_size=128, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T09:36:00.716016Z","iopub.execute_input":"2022-05-20T09:36:00.716271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Different Method","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\n\nimport numpy as np\nimport random\nimport sys","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:35:55.899648Z","iopub.execute_input":"2022-05-24T08:35:55.900783Z","iopub.status.idle":"2022-05-24T08:36:01.888825Z","shell.execute_reply.started":"2022-05-24T08:35:55.900664Z","shell.execute_reply":"2022-05-24T08:36:01.888034Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#LOAD TEXT\n#Save notepad as UTF-8 (select from dropdown during saving)\nfilename = \"../input/game-of-thrones-text-generation/got1.txt\"\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\nprint(raw_text[0:1000])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:04.410747Z","iopub.execute_input":"2022-05-24T08:36:04.411432Z","iopub.status.idle":"2022-05-24T08:36:04.452167Z","shell.execute_reply.started":"2022-05-24T08:36:04.411385Z","shell.execute_reply":"2022-05-24T08:36:04.451267Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# data cleaning process\nimport re                                # Regular expressions to use sub function for replacing the useless text from the data\n\ndef clean_text(text):\n  text = re.sub(r',', '', text)\n  text = re.sub(r'\\'', '',  text)\n  text = re.sub(r'\\\"', '', text)\n  text = re.sub(r'\\(', '', text)\n  text = re.sub(r'\\)', '', text)\n  text = re.sub(r'\\n', '', text)\n  text = re.sub(r'“', '', text)\n  text = re.sub(r'”', '', text)\n  text = re.sub(r'’', '', text)\n  text = re.sub(r'\\.', '', text)\n  text = re.sub(r';', '', text)\n  text = re.sub(r':', '', text)\n  text = re.sub(r'\\-', '', text)\n\n  return text","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:06.463936Z","iopub.execute_input":"2022-05-24T08:36:06.464182Z","iopub.status.idle":"2022-05-24T08:36:06.473012Z","shell.execute_reply.started":"2022-05-24T08:36:06.464155Z","shell.execute_reply":"2022-05-24T08:36:06.472239Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# cleaning the data\nlower_data = raw_text.lower()           # Converting the string to lower case to get uniformity\n\nsplit_data = lower_data.splitlines()      # Splitting the data to get every line seperately but this will give the list of uncleaned data\n\nprint(split_data[0:100])                       \n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:08.713096Z","iopub.execute_input":"2022-05-24T08:36:08.714075Z","iopub.status.idle":"2022-05-24T08:36:08.725516Z","shell.execute_reply.started":"2022-05-24T08:36:08.714001Z","shell.execute_reply":"2022-05-24T08:36:08.724539Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"final = ''                                # initiating a argument with blank string to hold the values of final cleaned data\n\nfor line in split_data:\n  line = clean_text(line)\n  final += '\\n' + line\n\nprint(final[0:100])  ","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:11.941137Z","iopub.execute_input":"2022-05-24T08:36:11.941450Z","iopub.status.idle":"2022-05-24T08:36:12.362308Z","shell.execute_reply.started":"2022-05-24T08:36:11.941415Z","shell.execute_reply":"2022-05-24T08:36:12.360453Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"final_data = final.split('\\n')       # splitting again to get list of cleaned and splitted data ready to be processed\nprint(final_data[0:100]) ","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:14.451233Z","iopub.execute_input":"2022-05-24T08:36:14.452162Z","iopub.status.idle":"2022-05-24T08:36:14.462732Z","shell.execute_reply.started":"2022-05-24T08:36:14.452106Z","shell.execute_reply":"2022-05-24T08:36:14.461791Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Instantiating the Tokenizer\nmax_vocab = 50\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(final_data)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:17.062304Z","iopub.execute_input":"2022-05-24T08:36:17.062926Z","iopub.status.idle":"2022-05-24T08:36:17.906423Z","shell.execute_reply.started":"2022-05-24T08:36:17.062882Z","shell.execute_reply":"2022-05-24T08:36:17.905479Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Getting the total number of words of the data.\nword2idx = tokenizer.word_index\nprint(len(word2idx))\n###print(word2idx)\nvocab_size = len(word2idx) + 1        # Adding 1 to the vocab_size because the index starts from 1 not 0. This will make it uniform when using it further\n###print(vocab_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:18.746973Z","iopub.execute_input":"2022-05-24T08:36:18.747689Z","iopub.status.idle":"2022-05-24T08:36:18.755358Z","shell.execute_reply.started":"2022-05-24T08:36:18.747652Z","shell.execute_reply":"2022-05-24T08:36:18.754552Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# We will turn the sentences to sequences line by line and create n_gram sequences\n\ninput_seq = []\n\nfor line in final_data:\n  token_list = tokenizer.texts_to_sequences([line])[0]\n  for i in range(1, len(token_list)):\n    n_gram_seq = token_list[:i+1]\n    input_seq.append(n_gram_seq)\n\n####print(input_seq)\n####print(input_seq[0:100])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:20.469285Z","iopub.execute_input":"2022-05-24T08:36:20.469961Z","iopub.status.idle":"2022-05-24T08:36:21.078551Z","shell.execute_reply.started":"2022-05-24T08:36:20.469924Z","shell.execute_reply":"2022-05-24T08:36:21.077769Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Getting the maximum length of sequence for padding purpose\nmax_seq_length = max(len(x) for x in input_seq)\nprint(max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:22.497924Z","iopub.execute_input":"2022-05-24T08:36:22.498225Z","iopub.status.idle":"2022-05-24T08:36:22.519006Z","shell.execute_reply.started":"2022-05-24T08:36:22.498190Z","shell.execute_reply":"2022-05-24T08:36:22.517812Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# Padding the sequences and converting them to array\ninput_seq = np.array(pad_sequences(input_seq, maxlen=max_seq_length, padding='pre'))\nprint(input_seq)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:24.406990Z","iopub.execute_input":"2022-05-24T08:36:24.407655Z","iopub.status.idle":"2022-05-24T08:36:24.909263Z","shell.execute_reply.started":"2022-05-24T08:36:24.407600Z","shell.execute_reply":"2022-05-24T08:36:24.907434Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Taking xs and labels to train the model.\n\nxs = input_seq[:, :-1]        # xs contains every word in sentence except the last one because we are using this value to predict the y value\nlabels = input_seq[:, -1]     # labels contains only the last word of the sentence which will help in hot encoding the y value in next step\nprint(\"xs: \",xs)\nprint(\"labels:\",labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:26.730254Z","iopub.execute_input":"2022-05-24T08:36:26.731009Z","iopub.status.idle":"2022-05-24T08:36:26.740728Z","shell.execute_reply.started":"2022-05-24T08:36:26.730944Z","shell.execute_reply":"2022-05-24T08:36:26.738655Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# one-hot encoding the labels according to the vocab size\n\n# The matrix is square matrix of the size of vocab_size. Each row will denote a label and it will have \n# a single +ve value(i.e 1) for that label and other values will be zero. \n\nys = to_categorical(labels, num_classes=vocab_size)\nprint(ys)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:28.872401Z","iopub.execute_input":"2022-05-24T08:36:28.872677Z","iopub.status.idle":"2022-05-24T08:36:29.124678Z","shell.execute_reply.started":"2022-05-24T08:36:28.872648Z","shell.execute_reply":"2022-05-24T08:36:29.123528Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:36:31.086156Z","iopub.execute_input":"2022-05-24T08:36:31.086837Z","iopub.status.idle":"2022-05-24T08:36:31.093319Z","shell.execute_reply.started":"2022-05-24T08:36:31.086801Z","shell.execute_reply":"2022-05-24T08:36:31.092442Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# using the pipeline method of sequential to define a model\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,64 , input_length=max_seq_length-1))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(20, return_sequences=True))\nmodel.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(vocab_size, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:37:06.096191Z","iopub.execute_input":"2022-05-24T08:37:06.096500Z","iopub.status.idle":"2022-05-24T08:37:06.907235Z","shell.execute_reply.started":"2022-05-24T08:37:06.096468Z","shell.execute_reply":"2022-05-24T08:37:06.906282Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001),\n              loss = 'categorical_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:37:08.784986Z","iopub.execute_input":"2022-05-24T08:37:08.785269Z","iopub.status.idle":"2022-05-24T08:37:08.798210Z","shell.execute_reply.started":"2022-05-24T08:37:08.785240Z","shell.execute_reply":"2022-05-24T08:37:08.797158Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.summary() ","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:37:14.886385Z","iopub.execute_input":"2022-05-24T08:37:14.886675Z","iopub.status.idle":"2022-05-24T08:37:14.900734Z","shell.execute_reply.started":"2022-05-24T08:37:14.886644Z","shell.execute_reply":"2022-05-24T08:37:14.899532Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"r = model.fit(xs,ys,epochs=30)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T08:37:22.694702Z","iopub.execute_input":"2022-05-24T08:37:22.695336Z","iopub.status.idle":"2022-05-24T09:24:47.569596Z","shell.execute_reply.started":"2022-05-24T08:37:22.695293Z","shell.execute_reply":"2022-05-24T09:24:47.568749Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model on accuracy\nimport matplotlib.pyplot as plt\nplt.plot(r.history['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T09:24:57.569076Z","iopub.execute_input":"2022-05-24T09:24:57.569352Z","iopub.status.idle":"2022-05-24T09:24:57.891204Z","shell.execute_reply.started":"2022-05-24T09:24:57.569323Z","shell.execute_reply":"2022-05-24T09:24:57.890210Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def predict_words(seed, no_words):\n  for i in range(no_words):\n    token_list = tokenizer.texts_to_sequences([seed])[0]\n    token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n    predicted = np.argmax(model.predict(token_list), axis=1)\n\n    new_word = ''\n\n    for word, index in tokenizer.word_index.items():\n      if predicted == index:\n        new_word = word\n        break\n    seed += \" \" + new_word\n  print(seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T09:25:04.100943Z","iopub.execute_input":"2022-05-24T09:25:04.101236Z","iopub.status.idle":"2022-05-24T09:25:04.108689Z","shell.execute_reply.started":"2022-05-24T09:25:04.101204Z","shell.execute_reply":"2022-05-24T09:25:04.107483Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# predicting or generating the poem with the seed text\n\nseed_text = 'will could see'\nnext_words = 15\n\npredict_words(seed_text, next_words)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T09:31:43.769597Z","iopub.execute_input":"2022-05-24T09:31:43.770291Z","iopub.status.idle":"2022-05-24T09:31:44.481814Z","shell.execute_reply.started":"2022-05-24T09:31:43.770253Z","shell.execute_reply":"2022-05-24T09:31:44.480927Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# predicting or generating the poem with the seed text\n\nseed_text = 'my wet nurse said'\nnext_words = 15\n\npredict_words(seed_text, next_words)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T09:55:49.220612Z","iopub.execute_input":"2022-05-24T09:55:49.220922Z","iopub.status.idle":"2022-05-24T09:55:49.843279Z","shell.execute_reply.started":"2022-05-24T09:55:49.220892Z","shell.execute_reply":"2022-05-24T09:55:49.842543Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# saving the model\n\nmodel.save('text_generator.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T09:55:54.274532Z","iopub.execute_input":"2022-05-24T09:55:54.275119Z","iopub.status.idle":"2022-05-24T09:56:03.194819Z","shell.execute_reply.started":"2022-05-24T09:55:54.275082Z","shell.execute_reply":"2022-05-24T09:56:03.193710Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#loading the model \nfrom keras.models import load_model\nmodel = load_model('text_generator.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T10:03:42.729684Z","iopub.execute_input":"2022-05-24T10:03:42.729986Z","iopub.status.idle":"2022-05-24T10:03:49.377452Z","shell.execute_reply.started":"2022-05-24T10:03:42.729955Z","shell.execute_reply":"2022-05-24T10:03:49.375949Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}